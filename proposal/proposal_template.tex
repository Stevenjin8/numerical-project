\documentclass[11pt]{article}
\usepackage[left=1.00in, right=1.00in, top=1.00in, bottom=1.00in]{geometry}
\usepackage{framed}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
%
\title{Project Proposal: \\ Latent Semantic Indexing}
%
\author{Cole Helgaas, Steven Jin}
\date{Oct 12, 2022}

\begin{document}
\maketitle
\textbf{Instructions:} Your proposal will consist of 3-4 paragraphs addressing the prompts below, followed by 4-5 formal references, entered into a bibtex file.
These references should be reputable sources (no wikipedia pages, youtube videos, or blog pages).
The template will automatically make these references appear at the end of this proposal (you can comment out the example references in the bibtex template file).
\begin{enumerate}
	\item \emph{Provide a description of topic.
      Explain your topic, its broader impact, and the role that SVD plays in this application.
    }

    Our project topic is Latent Semantic Indexing (LSI) which is a topic clustering algorithm.
    Deerwester et al. originally created LSI as an improvement on classical information retrieval algorithms \cite{seminal}.
    Rather than using lexical matches, which suffer from synonyms and polysemy, LSI captures the semantics behind the words allowing for better matching.
    LSI has been applied to tasks beyond language retrieval, such as cross-language information retrieval, modeling human memory, and generating graphs of terrorist networks [Wikipedia cite].

    LSI starts off with a $n \times m$ term-document matrix $X$ where $x_{i,j}$ represents how often word $i$ appears in document $j$.
    Here, $n$ is the number of unique words, and $m$ is the number of documents.
    We then normalize the matrix $X$ to $\tilde{X}$ by applying the tf-idf transformation [explain].
    Finally, we run SVD on $\tilde{X}$ to get
    \[
      \tilde{X} = T_0 \Sigma_0 D_0^{T}.
    \]
    The power of LSI comes from performing the dimensionality reduction
    \[
      \hat{X} = T \Sigma D^{T}
    \]
    where $T$ is $n \times k$, $\Sigma$ is $k \times k$ and $D$ is $k \times m$ for some $k < \min(n, m)$.
    The idea is that this reduction generates $k$ topics.
    The rows of $T$ represent word-topic associations (also known as embeddings).
    The columns of $D$ represent document-topic associations.
    The diagonal of $\Sigma$ represents the relative importance of each topic.
    Finally, we can generate an embedding $d$ for a new document $x$ with
    $d = \Sigma^{-1} T^{T} x$.
    We can compare the similarity of words or documents by taking the dot product of their topic associations weighted by the diagonal of $\Sigma^2$.

  \item
    \emph{Describe what you intend to accomplish in your paper.
      Look back on the SVD project page at the components every project must include.
      Explain what modifications/extensions you will do in this project that is different that what you are seeing in resources
      (how will this project be different/unique to your team?).
    }

    In this project, we will run LSI on course descriptions for courses offered at Middlebury for the 2022-23 academic year.
    We then will analyze the results to see if low-rank representations of course descriptions capture high-level attributes of courses.
    For example, we will look at whether low-rank representations discriminate STEM vs non-STEM classes.
    Further, we will inspect the topic vectors (columns of $T$) to see if the topics generated by LSI match topics high-level topics such as department.
    Finally, we will look for algorithmic bias by looking at word-topic associations for words associated with historically marginalized groups.
    Although there are works that examine algorithmic bias in modern natural language processing methods, we could not find any that did so for LSI.

	\item
    \emph{Give some ideas for what your team might do for the live script portion of this project.}

    For the interactive portion of this assignment, we will create a live script that lets the user input a query to find courses that match that query.
    The live script will also find the most related department to the query.
    We can generate department embeddings by averaging the course embeddings of courses offered by said department.
    Another idea is to let students pick some classes and see how they cluster in the topic space.

	\item
    \emph{\textbf{DEI + J:}  Think about the work we have done learning about diversity, equity, inclusion, and justice in math. 
      What are some proactive steps you might take in this project to create windows, mirrors, and sliding glass doors for your audience?
      Promote inclusion and/or equity?
    }

    Latent space models such as LSI uncover hidden and implicit patterns in our everyday lives.
    We hope to show that even innocent texts such as course descriptions inherit our biases.
    Hopefully, our work encourages departments to write more inclusively their course descriptions and beyond.
    We also hope to show students early in their math careers that the Math department at Middlebury is making DEI+J efforts and encourage these students to pursue mathematics.
\end{enumerate}
	%%%%%%%%%%%%%%%%%%%% HONOR CODE and COLLABORATION PLEDGE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{framed}
		\noindent \textbf{Honor Code Language on Duplicate Use of Work:}\\  \textit{Any work submitted to meet the requirements of a particular course is expected to be original work completed for that course. Students who wish to incorporate any portion of their own previously developed work into a new assignment must consult with the involved faculty members to establish appropriate expectations and parameters.} \\
		
		By signing below, you are indicating that the proposed project will be original work for this course.  If the proposed project overlaps with any previous work/experiences, you are required to discuss plans with Professor Kubacki before continuing.\\
		
		\noindent \textbf{Collaborative Work Pledge and Permission:}\\ I was an active collaborator on this assignment.  I approve of its final content.\\
		
		Include signatures below to indicate agreement with the above statements.
		\vspace{1.5in}
		\vspace{1.5in}
	\end{framed}
	\nocite{*}
	\bibliographystyle{plain}
	\bibliography{bibfile.bib} % you must have the correct name of your bib file here!
\end{document}
