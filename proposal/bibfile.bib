@article{seminal, author = {Scott Deerwester, Susan T. Dumais, Richard Harshman},
	title = {Indexing by Latent Semantic Analysis},
	journal = {Proceedings of the 51st Annual Meeting of the American Society for Information Science},
	year = {1988},
	volume = {25},
	pages = {36-40},
	annotate = {Seminal work on LSI},
}

@article{
  semantic-bias,
  author = {Aylin Caliskan, Joanna J. Bryson, Arvind Narayanan },
  title = {Semantics derived automatically from language corpora contain human-like biases},
  journal = {Science},
  volume = {356},
  number = {6334},
  pages = {183-186}, year = {2017},
  doi = {10.1126/science.aal4230},
  URL = {https://www.science.org/doi/abs/10.1126/science.aal4230},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.aal4230},
  abstract = {AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.}
}

@InBook{murphy,
	author = {Kevin P. Murphy},
	title = {Machine Learning: A Probabilistic Perspective},
	publisher = {The MIT press},
	year = {2012},
	chapter = {27.2.2},
	pages = {651},
}

@InBook{stanford,
	author = {Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze},
	title = {Introduction to Information Retrieval},
	publisher = {Cambridge University Press},
	year = {2009},
	chapter = {18},
	pages = {403-419},
}

@article{dumais,
  author = {Susan T. Dumais},
  title = {Latent semantic analysis},
  journal = {Annual Review of Information Science and Technology},
  volume = {38},
  number = {1},
  pages = {188-230},
  doi = {https://doi.org/10.1002/aris.1440380105},
  url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/aris.1440380105},
  eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/aris.1440380105},
  year = {2004}
}

